{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4005747-6aaf-4ff8-b3a6-cacd4f54921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using requests module for Data Collection -\n",
    "                     \n",
    "# Today we will see how to scrape websites and use requests module to download the raw html of a webpage.\n",
    "# In this section we can safely use https://quotes.toscrape.com/ and https://books.toscrape.com/ for scraping demos\n",
    "\n",
    "# 1. What is requests?\n",
    "# requests is a Python library used to send HTTP requests easily.\n",
    "# It allows you to fetch the content of a webpage programmatically.\n",
    "# It is commonly used as the first step before parsing HTML with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18d3250-e7ee-4ddc-b445-da652e1abc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Installing requests\n",
    "# To install requests, run:\n",
    "# pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27cd34e5-b679-47d0-b421-1d3c47b140c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sending a Basic GET Request\n",
    "# Example -\n",
    "\n",
    "# import requests\n",
    "\n",
    "# url = \"https://example.com\"\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # Print the HTML content\n",
    "# print(response.text)\n",
    "\n",
    "\n",
    "# Key points:\n",
    "# url: The website you want to fetch.\n",
    "# response.text: The HTML content of the page as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a7f2b1-4bc3-423a-9c6b-2fd944f6c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Checking the Response Status\n",
    "# Always check if the request was successful:\n",
    "\n",
    "# print(response.status_code)\n",
    "\n",
    "# Common Status Codes\n",
    "# 200: OK (Success)\n",
    "# 404: Not Found\n",
    "# 403: Forbidden\n",
    "# 500: Internal Server Error\n",
    "\n",
    "#-----------Best practice to do it using pythong code---------Saves time-----------------\n",
    "# if response.status_code == 200:\n",
    "#     print(\"Page fetched successfully!\")\n",
    "# else:\n",
    "#     print(\"Failed to fetch the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7b240a-6195-4bf2-9b62-24e3500b38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Important Response Properties -\n",
    "\n",
    "    \n",
    "# Property\t          Description\n",
    "    \n",
    "# response.text\t      HTML content as Unicode text\n",
    "# response.content\t  Raw bytes of the response\n",
    "# response.status_code  HTTP status code\n",
    "# response.headers\t  Metadata like content-type, server info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aecba38b-c664-44f9-98ee-1548c91f5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Best Practices for Fetching Pages\n",
    "# Always check the HTTP status code.\n",
    "# Use proper headers to mimic a browser.\n",
    "# Set a timeout to avoid hanging indefinitely.\n",
    "# Respect the website by not making too many rapid requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "596f2884-3441-4678-8833-8bfaa5df3ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Summary -\n",
    "\n",
    "# requests makes it simple to fetch web pages using Python.\n",
    "# It is the starting point for most web scraping workflows.\n",
    "# Combining requests with BeautifulSoup allows for powerful data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73a3bda-cb37-427f-975d-ba1ee70158dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice to download data from a website that you have to scrape :\n",
    "# below is the downloading part - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e664afe-01a1-440b-b13d-1f162d83f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page1 Downloaded successfully\n",
      "Page2 Downloaded successfully\n",
      "Page3 Downloaded successfully\n",
      "Page4 Downloaded successfully\n",
      "Page5 Downloaded successfully\n",
      "Page6 Downloaded successfully\n",
      "Page7 Downloaded successfully\n",
      "Page8 Downloaded successfully\n",
      "Page9 Downloaded successfully\n",
      "Page10 Downloaded successfully\n",
      "Page11 Downloaded successfully\n",
      "Page12 Downloaded successfully\n",
      "Page13 Downloaded successfully\n",
      "Page14 Downloaded successfully\n",
      "Page15 Downloaded successfully\n",
      "Page16 Downloaded successfully\n",
      "Page17 Downloaded successfully\n",
      "Page18 Downloaded successfully\n",
      "Page19 Downloaded successfully\n",
      "Page20 Downloaded successfully\n",
      "Page21 Downloaded successfully\n",
      "Page22 Downloaded successfully\n",
      "Page23 Downloaded successfully\n",
      "Page24 Downloaded successfully\n",
      "Page25 Downloaded successfully\n",
      "Page26 Downloaded successfully\n",
      "Page27 Downloaded successfully\n",
      "Page28 Downloaded successfully\n",
      "Page29 Downloaded successfully\n",
      "Page30 Downloaded successfully\n",
      "Page31 Downloaded successfully\n",
      "Page32 Downloaded successfully\n",
      "Page33 Downloaded successfully\n",
      "Page34 Downloaded successfully\n",
      "Page35 Downloaded successfully\n",
      "Page36 Downloaded successfully\n",
      "Page37 Downloaded successfully\n",
      "Page38 Downloaded successfully\n",
      "Page39 Downloaded successfully\n",
      "Page40 Downloaded successfully\n",
      "Page41 Downloaded successfully\n",
      "Page42 Downloaded successfully\n",
      "Page43 Downloaded successfully\n",
      "Page44 Downloaded successfully\n",
      "Page45 Downloaded successfully\n",
      "Page46 Downloaded successfully\n",
      "Page47 Downloaded successfully\n",
      "Page48 Downloaded successfully\n",
      "Page49 Downloaded successfully\n",
      "Page50 Downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "for i in range(1, 51):\n",
    "    response = requests.get(f\"https://books.toscrape.com/catalogue/page-{i}.html\")\n",
    "       \n",
    "    if response.status_code == 200:\n",
    "       with open(f\"htmls/page{i}.html\", \"w\", encoding= \"utf-8\") as f:\n",
    "          f.write(response.text)\n",
    "       print(f\"Page{i} Downloaded successfully\")    \n",
    "    else :\n",
    "       print(f\"Page{i} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0c205-3614-47b9-8203-09ef91b735b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
